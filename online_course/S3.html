<div class="row"><div class="col-sm-7 offset-1"><div id="txt-col"><h1 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section"><span class="ltx_text">3</span> </span><span class="ltx_text">Cost function minimization problems</span>
</h1><figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:NewtonProblem"><a href="online_course/images/NewtonMethod1D_problem01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F2.g1" src="online_course/images/small/NewtonMethod1D_problem01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 2</span>: </span><span class="ltx_text">Problem formulation.
</span></figcaption>
</figure><div class="ltx_para" id="S3-para:p1">
<p class="ltx_p">We would like the find the value of a decision variable <span class="ltx_Math" id="p1.m1">x</span> that would give us a cost <span class="ltx_Math" id="p1.m2">c(x)</span> that is a small as possible, see Figure <a class="ltx_ref" href="#S3-fig:NewtonProblem" title="Figure 2 ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">2</span></a>. Imagine that we start from an initial guess <span class="ltx_Math" id="p1.m3">x_{1}</span> and that can observe the behavior of this cost function within a very small region around our initial guess. Now let’s assume that we can make several consecutive guesses that will each time provide us with similar local information about the behavior of the cost around the points that we guessed. From this information, what point would you select as second guess (see question marks in the figure), based on the information that you obtained from the first guess?</p>
</div><div class="ltx_para" id="S3-para:p2">
<p class="ltx_p">There were two relevant information in the small portion of curve that we can observe in the figure to make a smart choice. First, the trend of the curve indicates that the cost seems to decrease if we move on the left side, and increase if we move on the right side. Namely, the slope <span class="ltx_Math" id="p2.m1">c^{\prime}(x_{1})</span> of the function <span class="ltx_Math" id="p2.m2">c(x)</span> at point <span class="ltx_Math" id="p2.m3">x_{1}</span> is positive. Second, we can observe that the portion of the curve has some curvature that can be also be informative about the way the trend of the curve <span class="ltx_Math" id="p2.m4">c(x)</span> will change by moving to the left or to the right. Namely, how much the slope <span class="ltx_Math" id="p2.m5">c^{\prime}(x_{1})</span> will change, corresponding to an acceleration <span class="ltx_Math" id="p2.m6">c^{\prime\prime}(x_{1})</span> around our first guess <span class="ltx_Math" id="p2.m7">x_{1}</span>. This is informative to estimate how much we should move to the left of the first guess to wisely select a second guess. 
<br class="ltx_break"/></p>
</div><div class="ltx_para" id="S3-para:p3">
<p class="ltx_p">Now that we have this intuition, we can move to a more formal problem formulation. Newton’s method attempts to solve <span class="ltx_Math" id="p3.m1">\min_{x}c(x)</span> or <span class="ltx_Math" id="p3.m2">\max_{x}c(x)</span> from an initial guess <span class="ltx_Math" id="p3.m3">x_{1}</span> by using a sequence of <span class="ltx_text ltx_font_bold">first-order Taylor approximations</span> (gradient descent) or <span class="ltx_text ltx_font_bold">second-order Taylor approximations</span> (Newton’s method) of <span class="ltx_Math" id="p3.m4">c(x)</span> around the iterates.</p>
</div><div class="ltx_subsection" id="S3-sec:SS1">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text">3.1</span> </span><span class="ltx_text">Gradient descent</span>
</h2>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:gradientDescent"><a href="online_course/images/gradientDescent1D01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F3.g1" src="online_course/images/small/gradientDescent1D01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 3</span>: </span><span class="ltx_text">Gradient descent for minimization, starting from an initial estimate <span class="ltx_Math" id="F3.m2">x_{1}</span> and converging to a local minimum (red point) after 8 iterations.
</span></figcaption>
</figure>
<figure class="ltx_float" id="S3-alg:linesearch">
<div class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div class="ltx_listingline"> <span class="ltx_Math" id="algorithm1.m5">\alpha\leftarrow 1</span>
</div>
<div class="ltx_listingline"> <span class="ltx_text ltx_font_bold">while</span> <em class="ltx_emph ltx_font_italic"><span class="ltx_Math" id="algorithm1.m6">c(\bm{x}+\alpha\;\Delta\bm{x})&gt;c(\bm{x})\;\textbf{and}\;\;\alpha&gt;\alpha_{\min}</span></em> <span class="ltx_text ltx_font_bold">do</span>
</div>
<div class="ltx_listingline">  <span class="ltx_rule"> </span>    <span class="ltx_Math" id="algorithm1.m7">\alpha\leftarrow\frac{\alpha}{2}</span>
</div>
<div class="ltx_listingline"> end while
</div>
<div class="ltx_listingline">
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold">Algorithm 1</span> </span>Backtracking line search method with parameter <span class="ltx_Math" id="algorithm1.m3">\alpha_{\min}</span> (presented here for decision variable <span class="ltx_Math" id="algorithm1.m4">\bm{x}</span>)</figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS1.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="#S3-fig:gradientDescent" title="Figure 3 ‣ 3.1 Gradient descent ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">3</span></a> shows how consecutive <span class="ltx_text ltx_font_bold">first-order Taylor approximations</span> of <span class="ltx_Math" id="SS1.p1.m1">c(x)</span> around the iterates allow to solve a minimization problem.</p>
</div>
<div class="ltx_para" id="S3-para:SS1.p2">
<p class="ltx_p">The first-order Taylor expansion around the point <span class="ltx_Math" id="SS1.p2.m1">x_{k}</span> can be expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex8">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex8.m1">c(x_{k}\!+\!\Delta x_{k})\approx c(x_{k})+c^{\prime}(x_{k})\;\Delta x_{k},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <span class="ltx_Math" id="SS1.p2.m2">c^{\prime}(x_{k})</span> is the derivative of <span class="ltx_Math" id="SS1.p2.m3">c(x_{k})</span> at point <span class="ltx_Math" id="SS1.p2.m4">x_{k}</span>.</p>
</div>
<div class="ltx_para" id="S3-para:SS1.p3">
<p class="ltx_p">By starting from a point <span class="ltx_Math" id="SS1.p3.m1">x_{k}</span> at each step <span class="ltx_Math" id="SS1.p3.m2">k</span>, we are interested in applying a correction <span class="ltx_Math" id="SS1.p3.m3">\Delta x_{k}</span> that would decrease the cost <span class="ltx_Math" id="SS1.p3.m4">c(x_{k})</span>.
If the cost function follows a linear trend at point <span class="ltx_Math" id="SS1.p3.m5">x_{k}</span> with a slope defined by its gradient <span class="ltx_Math" id="SS1.p3.m6">c^{\prime}(x_{k})</span>, one direction would increase the cost while the other would decrease it. Thus, by applying a correction <span class="ltx_Math" id="SS1.p3.m7">\Delta x_{k}=-\alpha c^{\prime}(x_{k})</span>, where <span class="ltx_Math" id="SS1.p3.m8">\alpha</span> is a positive scaling factor, we go down the slope estimated at <span class="ltx_Math" id="SS1.p3.m9">x_{k}</span>.</p>
</div>
<div class="ltx_para" id="S3-para:SS1.p4">
<p class="ltx_p">The scaling factor <span class="ltx_Math" id="SS1.p4.m1">\alpha</span> can be either constant or variable. If <span class="ltx_Math" id="SS1.p4.m2">\alpha</span> is too large, there is the risk that our local linear approximation is not valid anymore when we move far away from <span class="ltx_Math" id="SS1.p4.m3">x_{k}</span>. If it is too small, the iterative algorithm will require many iteration steps to converge to a local minimum of the cost function.</p>
</div>
<div class="ltx_para" id="S3-para:SS1.p5">
<p class="ltx_p">In practice, a simple backtracking line search procedure can be considered with Algorithm <a class="ltx_ref" href="#S3-alg:linesearch" title="Algorithm 1 ‣ 3.1 Gradient descent ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">1</span></a>, by considering a small value for <span class="ltx_Math" id="SS1.p5.m1">\alpha_{\min}</span>, see Figure <a class="ltx_ref" href="#S3-fig:linesearch" title="Figure 4 ‣ Multidimensional case ‣ 3.1 Gradient descent ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">4</span></a>. For more elaborated methods, see Ch. 3 of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="#bib-bib:bib19" title="Numerical optimization">13</a>]</cite>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_subsubsection" id="S3-sec:SS1.SSSx1">
<h3 class="ltx_title ltx_title_subsubsection">Multidimensional case</h3>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:linesearch"><a href="online_course/images/linesearch01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F4.g1" src="online_course/images/small/linesearch01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 4</span>: </span><span class="ltx_text">backtracking line search to scale the update vector <span class="ltx_Math" id="F4.m5">\Delta x_{k}</span> until the update decreases the cost. In this example, by starting with <span class="ltx_Math" id="F4.m6">\alpha=1</span> and by iteratively dividing <span class="ltx_Math" id="F4.m7">\alpha</span> by two, the procedure provides a scaling factor <span class="ltx_Math" id="F4.m8">\alpha=0.25</span>.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS1.SSSx1.p1">
<p class="ltx_p">For functions that depend on multiple variables stored as multidimensional vectors <span class="ltx_Math" id="SS1.SSSx1.p1.m1">\bm{x}</span>, the cost function <span class="ltx_Math" id="SS1.SSSx1.p1.m2">c(\bm{x})</span> can similarly be approximated by a first-order Taylor expansion around the point <span class="ltx_Math" id="SS1.SSSx1.p1.m3">\bm{x}_{k}</span> with
</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex9">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex9.m1">c(\bm{x}_{k}\!+\!\Delta\bm{x}_{k})\approx c(\bm{x}_{k})+{\bm{g}(\bm{x}_{k})}^{\scriptscriptstyle\top}\Delta\bm{x}_{k},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">with gradient vector</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex10">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex10.m1">\bm{g}(\bm{x}_{k})=\frac{\partial c}{\partial\bm{x}}\Big|_{\bm{x}_{k}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS1.SSSx1.p2">
<p class="ltx_p">By starting from an initial estimate <span class="ltx_Math" id="SS1.SSSx1.p2.m1">\bm{x}_{1}</span> and by recursively refining the current estimate by following the gradient, we obtain at each iteration <span class="ltx_Math" id="SS1.SSSx1.p2.m2">k</span> the recursion</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex11">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex11.m1">\bm{x}_{k+1}=\bm{x}_{k}-\alpha\bm{g}(\bm{x}_{k}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
</div>
</div><div class="ltx_subsection" id="S3-sec:SS2">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text">3.2</span> </span><span class="ltx_text">Newton’s method</span>
</h2>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:Newton"><a href="online_course/images/NewtonMethod1D01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F5.g1" src="online_course/images/small/NewtonMethod1D01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 5</span>: </span><span class="ltx_text">Newton’s method for minimization, starting from an initial estimate <span class="ltx_Math" id="F5.m2">x_{1}</span> and converging to a local minimum (red point) after 5 iterations.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS2.p1">
<p class="ltx_p">Figure <a class="ltx_ref" href="#S3-fig:Newton" title="Figure 5 ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">5</span></a> shows how consecutive <span class="ltx_text ltx_font_bold">second-order Taylor approximations</span> of <span class="ltx_Math" id="SS2.p1.m1">c(x)</span> around the iterates allow to solve a minimization problem.</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p2">
<p class="ltx_p">The second-order Taylor expansion around the point <span class="ltx_Math" id="SS2.p2.m1">x_{k}</span> can be expressed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_1D">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E4.m1">c(x_{k}\!+\!\Delta x_{k})\approx c(x_{k})+c^{\prime}(x_{k})\;\Delta x_{k}+\frac{1}{2}c^{\prime\prime}(x_{k})\;\Delta x_{k}^{2},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr>
</table>
<p class="ltx_p">where <span class="ltx_Math" id="SS2.p2.m2">c^{\prime}(x_{k})</span> and <span class="ltx_Math" id="SS2.p2.m3">c^{\prime\prime}(x_{k})</span> are the first and second derivatives at point <span class="ltx_Math" id="SS2.p2.m4">x_{k}</span>.</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p3">
<p class="ltx_p">We are interested in solving minimization problems with this approximation. If the second derivative <span class="ltx_Math" id="SS2.p3.m1">c^{\prime\prime}(x_{k})</span> is positive, the quadratic approximation is a convex function of <span class="ltx_Math" id="SS2.p3.m2">\Delta x_{k}</span>, and its minimum can be found by setting the derivative to zero.
</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p4">
<p class="ltx_p">Indeed, to find the local optima of a function, we can localize the points whose derivatives are zero (horizontal slopes), see Figure <a class="ltx_ref" href="#S3-fig:optimPrinciple" title="Figure 6 ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">6</span></a> for an illustration.</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p5">
<p class="ltx_p">By differentiating (<a class="ltx_ref" href="#S3-eq:Taylor_1D" title="(4) ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">4</span></a>) w.r.t. <span class="ltx_Math" id="SS2.p5.m1">\Delta x_{k}</span> and equating to zero, we then obtain</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex12">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex12.m1">c^{\prime}(x_{k})+c^{\prime\prime}(x_{k})\,\Delta x_{k}=0,</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">meaning that the minimum is given by</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex13">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex13.m1">\Delta\hat{x}_{k}=-\frac{c^{\prime}(x_{k})}{c^{\prime\prime}(x_{k})}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">which corresponds to the offset to apply to <span class="ltx_Math" id="SS2.p5.m2">x_{k}</span> to minimize the second-order polynomial approximation of the cost at this point.</p>
</div>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:optimPrinciple"><a href="online_course/images/optim_principle01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F6.g1" src="online_course/images/small/optim_principle01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 6</span>: </span><span class="ltx_text">Finding local optima.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS2.p6">
<p class="ltx_p">By starting from an initial estimate <span class="ltx_Math" id="SS2.p6.m1">x_{1}</span> and by recursively refining the current estimate by computing the offset that would minimize the polynomial approximation of the cost at the current estimate, we obtain at each iteration <span class="ltx_Math" id="SS2.p6.m2">k</span> the recursion</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_1D_update">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E5.m1">x_{k+1}=x_{k}-\frac{c^{\prime}(x_{k})}{c^{\prime\prime}(x_{k})}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr>
</table>
</div>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:NewtonNegativeHessian"><a href="online_course/images/NewtonMethod_negativeHessian01.png" target="_blank"><img class="ltx_graphics ltx_centering" id="F7.g1" src="online_course/images/small/NewtonMethod_negativeHessian01.png"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 7</span>: </span><span class="ltx_text">Newton update that would be achieved when the second derivative is negative.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS2.p7">
<p class="ltx_p">It is important that at each iteration, <span class="ltx_Math" id="SS2.p7.m1">c^{\prime\prime}(x_{k})</span> is positive, as we want to find local minima, see Figure <a class="ltx_ref" href="#S3-fig:NewtonNegativeHessian" title="Figure 7 ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">7</span></a> for an illustration. If <span class="ltx_Math" id="SS2.p7.m2">c^{\prime\prime}(x_{k})</span> is negative or too close to 0, it is in practice replaced by as small positive value in (<a class="ltx_ref" href="#S3-eq:Taylor_1D_update" title="(5) ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p8">
<p class="ltx_p">The geometric interpretation of Newton’s method is that at each iteration, it amounts to the fitting of a paraboloid to the surface of <span class="ltx_Math" id="SS2.p8.m1">c(x)</span> at <span class="ltx_Math" id="SS2.p8.m2">x_{k}</span>, having the same slopes and curvature as the surface at that point, and then proceeding to the maximum or minimum of that paraboloid. Note that if <span class="ltx_Math" id="SS2.p8.m3">c(x)</span> is a quadratic function, then the exact extremum is found in one step, which corresponds to the resolution of a least-squares problem.
</p>
</div>
<div class="ltx_para" id="S3-para:SS2.p9">
<p class="ltx_p">Similarly as for gradient descent, in practice, a simple backtracking line search procedure can also be considered with Algorithm <a class="ltx_ref" href="#S3-alg:linesearch" title="Algorithm 1 ‣ 3.1 Gradient descent ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_subsubsection" id="S3-sec:SS2.SSSx1">
<h3 class="ltx_title ltx_title_subsubsection">Multidimensional case</h3>
<figure class="ltx_figure ltx_align_floatright figure_main" id="S3-fig:Newton2D"><a href="online_course/images/NewtonMethod2D01.jpg" target="_blank"><img class="ltx_graphics ltx_centering" id="F8.g1" src="online_course/images/small/NewtonMethod2D01.jpg"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 8</span>: </span><span class="ltx_text">Newton’s method for minimization with 2D decision variables.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p1">
<p class="ltx_p">For functions that depend on multiple variables stored as multidimensional vectors <span class="ltx_Math" id="SS2.SSSx1.p1.m1">\bm{x}</span>, the cost function <span class="ltx_Math" id="SS2.SSSx1.p1.m2">c(\bm{x})</span> can similarly be approximated by a second-order Taylor expansion around the point <span class="ltx_Math" id="SS2.SSSx1.p1.m3">\bm{x}_{k}</span> with</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_nD">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E6.m1">c(\bm{x}_{k}\!+\!\Delta\bm{x}_{k})\approx c(\bm{x}_{k})+\Delta\bm{x}_{k}^{\scriptscriptstyle\top}\,\frac{\partial c}{\partial\bm{x}}\Big|_{\bm{x}_{k}}+\frac{1}{2}\Delta\bm{x}_{k}^{\scriptscriptstyle\top}\,\frac{\partial^{2}c}{\partial\bm{x}^{2}}\Big|_{\bm{x}_{k}}\Delta\bm{x}_{k},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr>
</table>
<p class="ltx_p">which can also be rewritten in augmented vector form as</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex14">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex14.m1">c(\bm{x}_{k}\!+\!\Delta\bm{x}_{k})\approx c(\bm{x}_{k})+\frac{1}{2}\begin{bmatrix}1\\
\Delta\bm{x}_{k}\end{bmatrix}^{\!{\scriptscriptstyle\top}}\begin{bmatrix}0&amp;\bm{g}_{\bm{x}}^{\scriptscriptstyle\top}\\
\bm{g}_{\bm{x}}&amp;\bm{H}_{\bm{x}\bm{x}}\end{bmatrix}\begin{bmatrix}1\\
\Delta\bm{x}_{k}\end{bmatrix},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">with gradient vector</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_grad">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E7.m1">\bm{g}(\bm{x}_{k})=\frac{\partial c}{\partial\bm{x}}\Big|_{\bm{x}_{k}},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr>
</table>
<p class="ltx_p">and Hessian matrix</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_Hess">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E8.m1">\bm{H}(\bm{x}_{k})=\frac{\partial^{2}c}{\partial\bm{x}^{2}}\Big|_{\bm{x}_{k}}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p2">
<p class="ltx_p">We are interested in solving minimization problems with this approximation. If the Hessian matrix <span class="ltx_Math" id="SS2.SSSx1.p2.m1">\bm{H}(\bm{x}_{k})</span> is positive definite, the quadratic approximation is a convex function of <span class="ltx_Math" id="SS2.SSSx1.p2.m2">\Delta\bm{x}_{k}</span>, and its minimum can be found by setting the derivatives to zero, see Figure <a class="ltx_ref" href="#S3-fig:Newton2D" title="Figure 8 ‣ Multidimensional case ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p3">
<p class="ltx_p">By differentiating (<a class="ltx_ref" href="#S3-eq:Taylor_nD" title="(6) ‣ Multidimensional case ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">6</span></a>) w.r.t. <span class="ltx_Math" id="SS2.SSSx1.p3.m1">\Delta\bm{x}_{k}</span> and equation to zero, we obtain that</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex15">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex15.m1">\Delta\bm{\hat{x}}_{k}=-{\bm{H}(\bm{x}_{k})}^{-1}\,\bm{g}(\bm{x}_{k}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">is the offset to apply to <span class="ltx_Math" id="SS2.SSSx1.p3.m2">\bm{x}_{k}</span> to minimize the second-order polynomial approximation of the cost at this point.</p>
</div>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p4">
<p class="ltx_p">By starting from an initial estimate <span class="ltx_Math" id="SS2.SSSx1.p4.m1">\bm{x}_{1}</span> and by recursively refining the current estimate by computing the offset that would minimize the polynomial approximation of the cost at the current estimate, we obtain at each iteration <span class="ltx_Math" id="SS2.SSSx1.p4.m2">k</span> the recursion</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Taylor_nD_update">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E9.m1">\bm{x}_{k+1}=\bm{x}_{k}-{\bm{H}(\bm{x}_{k})}^{-1}\,\bm{g}(\bm{x}_{k}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p5">
<p class="ltx_p">In practice, we need to verify if the Hessian matrix <span class="ltx_Math" id="SS2.SSSx1.p5.m1">\bm{H}(\bm{x}_{k})</span> is positive definite at each iteration step.</p>
</div>
<figure class="ltx_figure figure_main" id="S3-fig:gradientDescent_vs_NewtonMethod"><a href="online_course/images/gradientDescent_vs_NewtonMethod01.jpg" target="_blank"><img class="ltx_graphics ltx_centering" id="F9.g1" src="online_course/images/small/gradientDescent_vs_NewtonMethod01.jpg"/></a>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text">Figure 9</span>: </span><span class="ltx_text">Convergence of gradient descent and Newton’s method to solve minimization problems.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3-para:SS2.SSSx1.p6">
<p class="ltx_p">Figure <a class="ltx_ref" href="#S3-fig:gradientDescent_vs_NewtonMethod" title="Figure 9 ‣ Multidimensional case ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">9</span></a> shows the iteration steps taken by gradient descent and Newton’s method to solve two minimization problems. The minimization problem in the first row is a quadratic cost function, where Newton’s method converges in one iteration, while gradient descent requires multiple oscillatory steps to reach the minimum.</p>
</div>
</div>
</div><div class="ltx_subsection" id="S3-sec:GaussNewton">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text">3.3</span> </span><span class="ltx_text">Gauss–Newton algorithm</span>
</h2>
<div class="ltx_para" id="S3-para:SS3.p1">
<p class="ltx_p">The Gauss–Newton algorithm is a special case of Newton’s method in which the cost is quadratic (sum of squared function values), with the scalar cost <span class="ltx_Math" id="SS3.p1.m1">c(\bm{x})=\|\bm{f}(\bm{x})\|^{2}=\bm{f}(\bm{x})^{\scriptscriptstyle\top}\bm{f}(\bm{x})=\sum_{i=1}^{R}f_{i}^{2}(\bm{x})</span>, where <span class="ltx_Math" id="SS3.p1.m2">\bm{f}(\bm{x})\in\mathbb{R}^{R}</span> is a residual vector. By neglecting the second-order derivative terms, the gradient and Hessian can be computed with</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex16">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex16.m1">\bm{g}(\bm{x})=2\bm{J}(\bm{x})^{\scriptscriptstyle\top}\bm{f}(\bm{x}),\quad\bm{H}(\bm{x})\approx 2\bm{J}(\bm{x})^{\scriptscriptstyle\top}\bm{J}(\bm{x}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">where <span class="ltx_Math" id="SS3.p1.m3">\bm{J}(\bm{x})\in\mathbb{R}^{R\times D}</span> is the Jacobian matrix of <span class="ltx_Math" id="SS3.p1.m4">\bm{f}(\bm{x})</span>. This definition of the Hessian matrix makes it positive definite, which is useful to solve minimization problems as for well conditioned Jacobian matrices, we do not need to verify the positive definiteness of the Hessian matrix at each iteration.</p>
</div>
<div class="ltx_para" id="S3-para:SS3.p2">
<p class="ltx_p">The update rule in (<a class="ltx_ref" href="#S3-eq:Taylor_nD_update" title="(9) ‣ Multidimensional case ‣ 3.2 Newton’s method ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">9</span></a>) then becomes</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx4">
<tbody id="S3-eq:Ex17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex17.m1">\displaystyle\bm{x}_{k+1}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex17.m2">\displaystyle=\bm{x}_{k}-{\big(\bm{J}^{\scriptscriptstyle\top}(\bm{x}_{k})\bm{J}(\bm{x}_{k})\big)}^{-1}\,\bm{J}^{\scriptscriptstyle\top}(\bm{x}_{k})\,\bm{f}(\bm{x}_{k})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:GaussNewtonUpdate"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="E10.m1">\displaystyle=\bm{x}_{k}-\bm{J}^{\dagger}(\bm{x}_{k})\,\bm{f}(\bm{x}_{k}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <span class="ltx_Math" id="SS3.p2.m1">\bm{J}^{\dagger}</span> denotes the pseudoinverse of <span class="ltx_Math" id="SS3.p2.m2">\bm{J}</span>.</p>
</div>
<div class="ltx_para" id="S3-para:SS3.p3">
<p class="ltx_p">For comparison, the corresponding gradient descent problem would be computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex18">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex18.m1">\bm{x}_{k+1}=\bm{x}_{k}-\,\bm{J}^{\scriptscriptstyle\top}(\bm{x}_{k})\,\bm{f}(\bm{x}_{k}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS3.p4">
<p class="ltx_p">Thus, for costs that can be expressed as <span class="ltx_Math" id="SS3.p4.m1">c(\bm{x})=\bm{f}(\bm{x})^{\scriptscriptstyle\top}\bm{f}(\bm{x})</span>, the difference between first order or second order optimization is that the latter transforms the gradient of the former by the inverse of the Hessian matrix <span class="ltx_Math" id="SS3.p4.m2">\bm{H}(\bm{x}_{k})=\bm{J}^{\scriptscriptstyle\top}(\bm{x}_{k})\bm{J}(\bm{x}_{k})</span>. Another way to described this difference is to explain the former as an approximation of the latter with an Hessian matrix defined as identity (i.e., <span class="ltx_Math" id="SS3.p4.m3">\bm{H}=\bm{I}</span>).</p>
</div>
<div class="ltx_para" id="S3-para:SS3.p5">
<p class="ltx_p">The Gauss–Newton algorithm is the workhorse of many robotics problems, including inverse kinematics and optimal control, as we will see later in the document.</p>
</div>
</div><div class="ltx_subsection" id="S3-sec:LS">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text">3.4</span> </span><span class="ltx_text">Least squares</span>
</h2>
<div class="ltx_para" id="S3-para:SS4.p1">
<p class="ltx_p">We show in this section the direct link between Gauss–Newton algorithm and least squares. Extensions of least square are also presented here, which can also directly be used for optimization problems.</p>
</div>
<div class="ltx_para" id="S3-para:SS4.p2">
<p class="ltx_p">When the cost <span class="ltx_Math" id="SS4.p2.m1">c(\bm{x})</span> is a quadratic function w.r.t. <span class="ltx_Math" id="SS4.p2.m2">\bm{x}</span>, the optimization problem can be solved directly, without requiring iterative steps. Indeed, for any matrix <span class="ltx_Math" id="SS4.p2.m3">\bm{A}</span> and vector <span class="ltx_Math" id="SS4.p2.m4">\bm{b}</span>, we can see that if</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:costquadratic">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E11.m1">c(\bm{x})=\|\bm{A}\bm{x}-\bm{b}\|^{2}=(\bm{A}\bm{x}-\bm{b})^{\scriptscriptstyle\top}(\bm{A}\bm{x}-\bm{b}),</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr>
</table>
<p class="ltx_p">deriving <span class="ltx_Math" id="SS4.p2.m5">c(\bm{x})</span> w.r.t. <span class="ltx_Math" id="SS4.p2.m6">\bm{x}</span> and equating to zero yields</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:costquadraticsolution">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E12.m1">\bm{A}\bm{x}-\bm{b}=\bm{0}\iff\bm{x}=(\bm{A}^{\scriptscriptstyle\top}\bm{A})^{-1}\bm{A}^{\scriptscriptstyle\top}\bm{b}=\bm{A}^{\dagger}\bm{b},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr>
</table>
<p class="ltx_p">which corresponds to the standard analytic least squares estimate. We will see later in the inverse kinematics section that for the complete solution can also include a nullspace structure.</p>
</div>
<div class="ltx_para" id="S3-para:SS4.p3">
<p class="ltx_p">In contrast to the Gauss–Newton algorithm presented in Section <a class="ltx_ref" href="#S3-sec:GaussNewton" title="3.3 Gauss–Newton algorithm ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">3.3</span></a>, the optimization problem in (<a class="ltx_ref" href="#S3-eq:costquadratic" title="(11) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">11</span></a>), described by a quadratic cost on the decision variable <span class="ltx_Math" id="SS4.p3.m1">\bm{x}</span>, admits the solution (<a class="ltx_ref" href="#S3-eq:costquadraticsolution" title="(12) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">12</span></a>) that can be computed directly without relying on an iterative procedure. We can observe that if we follow the Gauss–Newton procedure, the residual vector corresponding to the cost (<a class="ltx_ref" href="#S3-eq:costquadratic" title="(11) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">11</span></a>) is <span class="ltx_Math" id="SS4.p3.m2">\bm{f}=\bm{A}\bm{x}-\bm{b}</span> and its Jacobian matrix is <span class="ltx_Math" id="SS4.p3.m3">\bm{J}=\bm{A}</span>. By starting from an initial estimate <span class="ltx_Math" id="SS4.p3.m4">\bm{x}_{0}</span>, the Gauss–Newton update in (<a class="ltx_ref" href="#S3-eq:GaussNewtonUpdate" title="(10) ‣ 3.3 Gauss–Newton algorithm ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">10</span></a>) then takes the form</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx5">
<tbody id="S3-eq:Ex19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex19.m1">\displaystyle\bm{x}_{k+1}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex19.m2">\displaystyle=\bm{x}_{k}-\bm{A}^{\dagger}(\bm{A}\bm{x}_{k}-\bm{b})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:Ex20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex20.m1">\displaystyle=\bm{x}_{k}-(\bm{A}^{\scriptscriptstyle\top}\bm{A})^{-1}\bm{A}^{\scriptscriptstyle\top}(\bm{A}\bm{x}_{k}-\bm{b})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:Ex21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex21.m1">\displaystyle=\bm{x}_{k}-\cancel{(\bm{A}^{\scriptscriptstyle\top}\bm{A})^{-1}}\cancel{(\bm{A}^{\scriptscriptstyle\top}\bm{A})}\bm{x}_{k}+\bm{A}^{\dagger}\bm{b}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:Ex22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex22.m1">\displaystyle=\bm{A}^{\dagger}\bm{b},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">which converges in a single iteration, independently of the initial guess <span class="ltx_Math" id="SS4.p3.m5">\bm{x}_{0}</span>. Indeed, a cost that takes a quadratic form with respect to the decision variable can be soled in batch form (least squares solution), which will be a useful property that we will exploit later in the context of linear quadratic controllers.</p>
</div>
<div class="ltx_para" id="S3-para:SS4.p4">
<p class="ltx_p">A <span class="ltx_text ltx_font_bold">ridge regression</span> problem can similarly be defined as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx6">
<tbody id="S3-eq:Ex23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex23.m1">\displaystyle\bm{\hat{x}}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex23.m2">\displaystyle=\arg\min_{\bm{x}}\;\|\bm{A}\bm{x}-\bm{b}\|^{2}+\alpha\|\bm{x}\|^{2}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:regLS"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="E13.m1">\displaystyle=(\bm{A}^{\scriptscriptstyle\top}\bm{A}+\alpha\bm{I})^{-1}\bm{A}^{\scriptscriptstyle\top}\bm{b},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is also called <span class="ltx_text ltx_font_bold">penalized least squares</span>, <span class="ltx_text ltx_font_bold">robust regression</span>, <span class="ltx_text ltx_font_bold">damped least squares</span> or <span class="ltx_text ltx_font_bold">Tikhonov regularization</span>. In (<a class="ltx_ref" href="#S3-eq:regLS" title="(13) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">13</span></a>), <span class="ltx_Math" id="SS4.p4.m1">\bm{I}</span> denotes an identity matrix (diagonal matrix with 1 as elements in the diagonal). <span class="ltx_Math" id="SS4.p4.m2">\alpha</span> is typically a small scalar value acting as a regularization term when inverting the matrix <span class="ltx_Math" id="SS4.p4.m3">\bm{A}^{\scriptscriptstyle\top}\bm{A}</span>.</p>
</div>
<div class="ltx_para" id="S3-para:SS4.p5">
<p class="ltx_p">The cost can also be weighted by a matrix <span class="ltx_Math" id="SS4.p5.m1">\bm{W}</span>, providing the <span class="ltx_text ltx_font_bold">weighted least squares</span> solution</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx7">
<tbody id="S3-eq:Ex24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex24.m1">\displaystyle\bm{\hat{x}}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex24.m2">\displaystyle=\arg\min_{\bm{x}}\;\|\bm{A}\bm{x}-\bm{b}\|^{2}_{\bm{W}}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:Ex25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex25.m1">\displaystyle=\arg\min_{\bm{x}}\;(\bm{A}\bm{x}-\bm{b})^{\scriptscriptstyle\top}\bm{W}(\bm{A}\bm{x}-\bm{b})</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:WLS"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="E14.m1">\displaystyle=(\bm{A}^{\scriptscriptstyle\top}\bm{W}\bm{A})^{-1}\bm{A}^{\scriptscriptstyle\top}\bm{W}\bm{b}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3-para:SS4.p6">
<p class="ltx_p">By combining (<a class="ltx_ref" href="#S3-eq:regLS" title="(13) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">13</span></a>) and (<a class="ltx_ref" href="#S3-eq:WLS" title="(14) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">14</span></a>), a <span class="ltx_text ltx_font_bold">weighted ridge regression</span> problem can be defined as</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx8">
<tbody id="S3-eq:Ex26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex26.m1">\displaystyle\bm{\hat{x}}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex26.m2">\displaystyle=\arg\min_{\bm{x}}\;\|\bm{A}\bm{x}-\bm{b}\|^{2}_{\bm{W}}+\alpha\|\bm{x}\|^{2}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:regWLS"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="E15.m1">\displaystyle=(\bm{A}^{\scriptscriptstyle\top}\bm{W}\bm{A}+\alpha\bm{I})^{-1}\bm{A}^{\scriptscriptstyle\top}\bm{W}\bm{b}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3-para:SS4.p7">
<p class="ltx_p">If the matrices and vectors in (<a class="ltx_ref" href="#S3-eq:regWLS" title="(15) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">15</span></a>) have the structure</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex27">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex27.m1">\bm{W}=\begin{bmatrix}\bm{W}_{1}&amp;\bm{0}\\
\bm{0}&amp;\bm{W}_{2}\end{bmatrix},\quad\bm{A}=\begin{bmatrix}\bm{A}_{1}\\
\bm{A}_{2}\end{bmatrix},\quad\bm{b}=\begin{bmatrix}\bm{b}_{1}\\
\bm{b}_{2}\end{bmatrix},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">we can consider an optimization problem using only the first part of the matrices, yielding the estimate</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S3-eq:A5.EGx9">
<tbody id="S3-eq:Ex28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_Math" id="Ex28.m1">\displaystyle\bm{\hat{x}}</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="Ex28.m2">\displaystyle=\arg\min_{\bm{x}}\;\|\bm{A}_{1}\bm{x}-\bm{b}_{1}\|^{2}_{\bm{W}_{1}}+\alpha\|\bm{x}\|^{2}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3-eq:regWLS1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span class="ltx_Math" id="E16.m1">\displaystyle=(\bm{A}_{1}^{\scriptscriptstyle\top}\bm{W}_{1}\bm{A}_{1}+\alpha\bm{I})^{-1}\bm{A}_{1}^{\scriptscriptstyle\top}\bm{W}_{1}\bm{b}_{1}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is the same as the result of computing (<a class="ltx_ref" href="#S3-eq:regWLS" title="(15) ‣ 3.4 Least squares ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">15</span></a>) with <span class="ltx_Math" id="SS4.p7.m1">\bm{W}_{2}=\bm{0}</span>.</p>
</div>
</div><div class="ltx_subsection" id="S3-sec:LSconstraints">
<h2 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text">3.5</span> </span><span class="ltx_text">Least squares with constraints</span>
</h2>
<div class="ltx_para" id="S3-para:SS5.p1">
<p class="ltx_p">A constrained minimization problem of the form</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:LSconstraints">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="E17.m1">\min_{\bm{x}}\;(\bm{A}\bm{x}-\bm{b})^{\scriptscriptstyle\top}(\bm{A}\bm{x}-\bm{b}),\quad\text{s.t.}\quad\bm{C}\bm{x}=\bm{h},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr>
</table>
<p class="ltx_p">can also be solved analytically by considering a Lagrange multiplier variable <span class="ltx_Math" id="SS5.p1.m1">\bm{\lambda}</span> allowing us to rewrite the objective as
</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex29">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex29.m1">\min_{\bm{x},\bm{\lambda}}\;(\bm{A}\bm{x}-\bm{b})^{\scriptscriptstyle\top}(\bm{A}\bm{x}-\bm{b})+\bm{\lambda}^{\scriptscriptstyle\top}(\bm{C}\bm{x}-\bm{h}).</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS5.p2">
<p class="ltx_p">Differentiating with respect to <span class="ltx_Math" id="SS5.p2.m1">\bm{x}</span> and <span class="ltx_Math" id="SS5.p2.m2">\bm{\lambda}</span> and equating to zero yields</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex30">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex30.m1">\bm{A}^{\scriptscriptstyle\top}(\bm{A}\bm{x}-\bm{b})+\bm{C}^{\scriptscriptstyle\top}\bm{\lambda}=\bm{0},\quad\bm{C}\bm{x}-\bm{h}=\bm{0},</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">which can rewritten in matrix form as</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex31">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex31.m1">\begin{bmatrix}\bm{A}^{\scriptscriptstyle\top}\!\bm{A}&amp;\bm{C}^{\scriptscriptstyle\top}\\
\bm{C}&amp;\bm{0}\end{bmatrix}\begin{bmatrix}\bm{x}\\
\bm{\lambda}\end{bmatrix}=\begin{bmatrix}\bm{A}^{\scriptscriptstyle\top}\bm{b}\\
\bm{h}\end{bmatrix}.</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
</div>
<div class="ltx_para" id="S3-para:SS5.p3">
<p class="ltx_p">With this augmented state representation, we can then see that</p>
<table class="ltx_equation ltx_eqn_table" id="S3-eq:Ex32">
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><span class="ltx_Math" id="Ex32.m1">\begin{bmatrix}\bm{x}\\
\bm{\lambda}\end{bmatrix}=\begin{bmatrix}\bm{A}^{\scriptscriptstyle\top}\!\bm{A}&amp;\bm{C}^{\scriptscriptstyle\top}\\
\bm{C}&amp;\bm{0}\end{bmatrix}^{-1}\begin{bmatrix}\bm{A}^{\scriptscriptstyle\top}\bm{b}\\
\bm{h}\end{bmatrix}</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</table>
<p class="ltx_p">minimizes the constrained cost. The first part of this augmented state then gives us the solution of (<a class="ltx_ref" href="#S3-eq:LSconstraints" title="(17) ‣ 3.5 Least squares with constraints ‣ 3 Cost function minimization problems ‣ A Math Cookbook for Robot Manipulation"><span class="ltx_text ltx_ref_tag">17</span></a>).
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</div><ul class="pagination justify-content-center small_menu"><li class="page-item previous_file"><a class="page-link" href="#S2">Previous</a></li><li class="page-item next_file"><a class="page-link" href="#S4">Next</a></li></ul></div></div><div class="col-sm-3"><div id="img-col"></div></div></div>